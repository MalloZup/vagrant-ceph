---
# This configuration file has two sections.  The first is the Vagrant boxes 
# subdivided into ceph installation methods with related repositories and 
# packages.  The second is the various server configurations to simulate 
# different environments.
#
########################################
# Installation Section
########################################
#
# The vagrant box is a JeOS image and has no initial repositories.  Create new
# entries to use your own repositories or add repositories to the existing lists.
#
# Change the INSTALLATION to either 'ceph-deploy' or 'vsm' in the Vagrantfile.
########################################
# OpenSUSE 13.2
########################################
openSUSE-13.2:
  ceph-deploy:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/distribution/13.2/repo/oss/'
      #'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_13.2/'
      'Ceph': 'http://download.suse.de/ibs/Devel:/Storage:/1.0/openSUSE_Factory/'
      'swiftgist/openSUSE 13.2': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_13.2/'
      'Devel C Libraries': 
        'http://download.opensuse.org/repositories/devel:libraries:c_c++/openSUSE_13.2/'
    packages:
      all:
        - ceph-deploy
        - vim
        - vim-data
      admin:
        - ceph-deploy
    commands:
      admin:
        - echo Done
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/distribution/13.2/repo/oss/'
      'swiftgist/openSUSE 13.2': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_13.2/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_13.2/'
      'Openstack - Juno': 'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/openSUSE_13.2/'
    packages:
      all:
        - vim
        - vim-data
        - lsb-release
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - curl
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
      data1: true
      data2: true
      data3: true
    commands:
      admin:
        - vsm-controller
        - ssh data1 vsm-storage
        - ssh data2 vsm-storage
        - ssh data3 vsm-storage
########################################
# OpenSUSE Tumbleweed
########################################
Tumbleweed:
  ceph-deploy:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/tumbleweed/repo/oss/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Tumbleweed/'
    packages:
      all:
        - vim
        - vim-data
      admin:
        - ceph-deploy
    commands:
      admin:
        - echo Done
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/tumbleweed/repo/oss/'
      # 'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Tumbleweed/'
      'Ceph IBS': 'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/openSUSE_Tumbleweed/'
      'swiftgist/openSUSE Tumbleweed': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_Tumbleweed/'
      'Openstack - Juno': 'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/openSUSE_Factory/'
    packages:
      all:
        - vim
        - vim-data
        - lsb-release
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - curl
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
      data1: true
      data2: true
      data3: true
    commands:
      admin:
        - vsm-controller
        - ssh data1 vsm-storage
        - ssh data2 vsm-storage
        - ssh data3 vsm-storage
########################################
# SLE 12
########################################
SLE-12:
  ceph-deploy:
    repos:
      'SLE12 Pool (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      'SLE12 Pool Updates (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/Update/standard/'
      'SDK':
        'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Products/SLE-SDK/12/x86_64/product'
      'SDK Updates':
        'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Updates/SLE-SDK/12/x86_64/update'
      #'Ceph': 
      #  'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
      #'Ceph': 
      #  'http://download.suse.de/ibs/Devel:/Storage:/1.0:/Staging/SLE_12/'
      'Ceph':
        'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/SLE12/'
      'Python Devel': 
        'http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12/'
    packages:
      all:
        - vim
        - vim-data
      admin:
        - ceph-deploy
    commands:
      admin:
        - echo Done
  salt:
    repos:
      'SLE12 Pool (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      'SLE12 Pool Updates (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/Update/standard/'
      'SDK':
        'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Products/SLE-SDK/12/x86_64/product'
      'SDK Updates':
        'http://euklid.suse.de/mirror/SuSE/build.suse.de/SUSE/Updates/SLE-SDK/12/x86_64/update'
      #'Ceph': 
      #  'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
      #'Ceph': 
      #  'http://download.suse.de/ibs/Devel:/Storage:/1.0:/Staging/SLE_12/'
      'Ceph':
        'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/SLE12/'
      'Python Devel': 
        'http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12/'
      'SES2 Staging': 
        'http://download.suse.de/ibs/Devel:/Storage:/2.0:/Staging/SLE12/'
    packages:
      all:
        - vim
        - vim-data
        - salt-minion
      admin:
        - ceph-deploy
    files:
      all: true
      admin: true
    commands:
      admin:
        #- rpm -e python-Twisted python-service_identity python-pyOpenSSL python-cryptography || exit 0
        #- rpm -Uvh --oldpackage /tmp/python-cffi-1.1.0-0.x86_64.rpm  || exit 0
        - zypper -n in --no-recommends salt-master
        - mv /etc/salt/master.d/transport-raet.conf /etc/salt/master.d/transport-raet.conf- || exit 0
        - systemctl start salt-master
        - systemctl enable salt-master
        - start_minions 
        - salt-key -y -A || exit 0
        - salt-run state.orchestrate orch.ceph
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      'Docker Devel': 'http://download.suse.de/ibs/Devel:/Docker/SLE_12/'
      'Python Devel': 
        'http://download.opensuse.org/repositories/devel:/languages:/python/SLE_12/'
      'Ceph': 
        'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
      'swiftgist/SLE_12': 
        'http://download.opensuse.org/repositories/home:/swiftgist/SLE_12/'
      'Openstack - Juno': 
        'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/SLE_12/'
    packages:
      all:
        - vim
        - vim-data
        - lsb-release
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - curl
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
      data1: true
      data2: true
      data3: true
    commands:
      admin:
        - vsm-controller
        - ssh data1 vsm-storage
        - ssh data2 vsm-storage
        - ssh data3 vsm-storage

########################################
# Configuration Section
########################################
#
# The default allows for reasonable exploration of Ceph installations and
# configurations.  Three monitors allow for a quorum and understanding failover,
# elections, etc.  Six data nodes with 30 osds and a pretend journal give
# enough flexibility to try replicas and erasure coding (e.g. k=3, m=2) while
# downing an osd or an entire node.
#
# This requires 5G RAM and ~32G disk space.  Initial vagrant up is ~28 minutes
# on a quad core.
default:
  description: "admin, 3 monitors, 6 data nodes with journals"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
      cluster: 172.16.2.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
      cluster: 172.16.2.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
    data5:
      management: 192.168.1.25
      public:  172.16.1.25
      cluster: 172.16.2.25
    data6:
      management: 192.168.1.26
      public:  172.16.1.26
      cluster: 172.16.2.26
  memory:
    admin: 1024
  disks:
    data1:
      hds: 5
      ssds: 1
    data2:
      hds: 5
      ssds: 1
    data3:
      hds: 5
      ssds: 1
    data4:
      hds: 5
      ssds: 1
    data5:
      hds: 5
      ssds: 1
    data6:
      hds: 5
      ssds: 1
# The small configuration is a typical demo setup of a monitor and three data
# nodes with two osds each.  This can be ideal for varifying that Vagrant and
# libvirt are behaving.  Simpler installations can be verified and the ceph
# command line can be explored.
#
# This requires 2.5G RAM and ~7G disk space.  Initial vagrant up is ~13 minutes
# on a quad core.
small:
  description: "admin, 1 monitor, 3 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
  memory:
    admin: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
iscsi:
  description: "admin, 2 clients, 3 monitors, 3 data nodes, 3 gateways"
  nodes:
    admin:
      management: 192.168.11.254
      public:  172.16.11.254
      cluster: 172.16.12.254
    client1:
      management: 192.168.11.5
      public:  172.16.11.5
      cluster: 172.16.12.5
    client2:
      management: 192.168.11.6
      public:  172.16.11.6
      cluster: 172.16.12.6
    mon1:
      management: 192.168.11.11
      public:  172.16.11.11
      cluster: 172.16.12.11
    mon2:
      management: 192.168.11.12
      public:  172.16.11.12
      cluster: 172.16.12.12
    mon3:
      management: 192.168.11.13
      public:  172.16.11.13
      cluster: 172.16.12.13
    igw1:
      management: 192.168.11.16
      public:  172.16.11.16
      cluster: 172.16.12.16
    igw2:
      management: 192.168.11.17
      public:  172.16.11.17
      cluster: 172.16.12.17
    igw3:
      management: 192.168.11.18
      public:  172.16.11.18
      cluster: 172.16.12.18
    data1:
      management: 192.168.11.21
      public:  172.16.11.21
      cluster: 172.16.12.21
    data2:
      management: 192.168.11.22
      public:  172.16.11.22
      cluster: 172.16.12.22
    data3:
      management: 192.168.11.23
      public:  172.16.11.23
      cluster: 172.16.12.23
  memory:
    admin: 1024
    igw1: 768
    igw2: 768
    igw3: 768
    client1: 768
    client2: 768
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
# The economical configuration represents a cold storage configuration that has
# no SSD journals.  Additionally, the monitors and admin node do not have an
# interface on the cluster network to save money on one less 10G interface.
#
# This requires 4G RAM and ~17G disk space.  Initial vagrant up is ~17 minutes
# on a quad core.
economical:
  description: "admin, 3 monitors, 4 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
  memory:
    admin: 1024
  disks:
    data1:
      hds: 4
    data2:
      hds: 4
    data3:
      hds: 4
    data4:
      hds: 4
