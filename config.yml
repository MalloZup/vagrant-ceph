---
# This configuration file has two sections.  The first is the Vagrant boxes 
# subdivided into ceph installation methods with related repositories and 
# packages.  The second is the various server configurations to simulate 
# different environments.
#
########################################
# Installation Section
########################################
#
# The vagrant box is a JeOS image and has no initial repositories.  Create new
# entries to use your own repositories or add repositories to the existing lists.
#
# Change the INSTALLATION to either 'ceph-deploy' or 'vsm' in the Vagrantfile.
########################################
# OpenSUSE 13.2
########################################
VagrantBox-openSUSE-13.2:
  ceph-deploy:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/distribution/13.2/repo/oss/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_13.2/'
    packages:
      all:
        - ceph-deploy
        - vim
        - vim-data
    commands:
      admin:
        - echo Done
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/distribution/13.2/repo/oss/'
      'swiftgist/openSUSE 13.2': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_13.2/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_13.2/'
      'Openstack - Juno': 'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/openSUSE_13.2/'
    packages:
      all:
        - vim
        - vim-data
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
    commands:
      admin:
        - vsm-controller
        - "grep -rl '^from django.conf.urls.defaults' /usr/share/vsm-dashboard/ | xargs sed -i 's!^from django.conf.urls.defaults!from django.conf.urls!'"
########################################
# OpenSUSE Tumbleweed
########################################
VagrantBox-Tumbleweed:
  ceph-deploy:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/tumbleweed/repo/oss/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Tumbleweed/'
    packages:
      all:
        - ceph-deploy
        - vim
        - vim-data
    commands:
      admin:
        - echo Done
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 'http://download.opensuse.org/tumbleweed/repo/oss/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/openSUSE_Tumbleweed/'
      'swiftgist/openSUSE Tumbleweed': 'http://download.opensuse.org/repositories/home:/swiftgist/openSUSE_Tumbleweed/'
      'Openstack - Juno': 'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/openSUSE_Factory/'
    packages:
      all:
        - vim
        - vim-data
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
    commands:
      admin:
        - vsm-controller
        - "grep -rl '^from django.conf.urls.defaults' /usr/share/vsm-dashboard/ | xargs sed -i 's!^from django.conf.urls.defaults!from django.conf.urls!'"
########################################
# SLE 12
########################################
VagrantBox-SLE12:
  ceph-deploy:
    repos:
      'Main Repository (OSS)': 'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      'Ceph': 'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
    packages:
      all:
        - ceph-deploy
        - vim
        - vim-data
    commands:
      admin:
        - echo Done
  # This installer requires an additional ~400M and ~4 minutes per VM initially.
  vsm:
    repos:
      'Main Repository (OSS)': 
        'http://download.suse.de/ibs/SUSE:/SLE-12:/GA/standard/'
      'Docker Devel': 'http://download.suse.de/ibs/Devel:/Docker/SLE_12/'
      'Ceph': 
        'http://download.opensuse.org/repositories/filesystems:/ceph/SLE_12/'
      'swiftgist/SLE_12': 
        'http://download.opensuse.org/repositories/home:/swiftgist/SLE_12/'
      'Openstack - Juno': 
        'http://download.opensuse.org/repositories/Cloud:/OpenStack:/Juno/SLE_12/'
    packages:
      all:
        - vim
        - vim-data
        - python-vsmclient
        - vsm
        - vsm-dashboard
        - vsm-deploy
      admin:
        - rabbitmq-server
        - openstack-keystone
    files:
      admin: true
    commands:
      admin:
        - vsm-controller
        - "grep -rl '^from django.conf.urls.defaults' /usr/share/vsm-dashboard/ | xargs sed -i 's!^from django.conf.urls.defaults!from django.conf.urls!'"

########################################
# Configuration Section
########################################
#
# The default allows for reasonable exploration of Ceph installations and
# configurations.  Three monitors allow for a quorum and understanding failover,
# elections, etc.  Six data nodes with 30 osds and a pretend journal give
# enough flexibility to try replicas and erasure coding (e.g. k=3, m=2) while
# downing an osd or an entire node.
#
# This requires 5G RAM and ~32G disk space.  Initial vagrant up is ~28 minutes
# on a quad core.
default:
  description: "admin, 3 monitors, 6 data nodes with journals"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
      cluster: 172.16.2.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
      cluster: 172.16.2.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
    data5:
      management: 192.168.1.25
      public:  172.16.1.25
      cluster: 172.16.2.25
    data6:
      management: 192.168.1.26
      public:  172.16.1.26
      cluster: 172.16.2.26
  memory:
    admin: 1024
  disks:
    data1:
      hds: 5
      ssds: 1
    data2:
      hds: 5
      ssds: 1
    data3:
      hds: 5
      ssds: 1
    data4:
      hds: 5
      ssds: 1
    data5:
      hds: 5
      ssds: 1
    data6:
      hds: 5
      ssds: 1
# The small configuration is a typical demo setup of a monitor and three data
# nodes with two osds each.  This can be ideal for varifying that Vagrant and
# libvirt are behaving.  Simpler installations can be verified and the ceph
# command line can be explored.
#
# This requires 2.5G RAM and ~7G disk space.  Initial vagrant up is ~13 minutes
# on a quad core.
small:
  description: "admin, 1 monitor, 3 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
      cluster: 172.16.2.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
      cluster: 172.16.2.11
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
  memory:
    admin: 1024
  disks:
    data1:
      hds: 2
    data2:
      hds: 2
    data3:
      hds: 2
# The economical configuration represents a cold storage configuration that has
# no SSD journals.  Additionally, the monitors and admin node do not have an
# interface on the cluster network to save money on one less 10G interface.
#
# This requires 4G RAM and ~17G disk space.  Initial vagrant up is ~17 minutes
# on a quad core.
economical:
  description: "admin, 3 monitors, 4 data nodes"
  nodes:
    admin:
      management: 192.168.1.254
      public:  172.16.1.254
    mon1:
      management: 192.168.1.11
      public:  172.16.1.11
    mon2:
      management: 192.168.1.12
      public:  172.16.1.12
    mon3:
      management: 192.168.1.13
      public:  172.16.1.13
    data1:
      management: 192.168.1.21
      public:  172.16.1.21
      cluster: 172.16.2.21
    data2:
      management: 192.168.1.22
      public:  172.16.1.22
      cluster: 172.16.2.22
    data3:
      management: 192.168.1.23
      public:  172.16.1.23
      cluster: 172.16.2.23
    data4:
      management: 192.168.1.24
      public:  172.16.1.24
      cluster: 172.16.2.24
  memory:
    admin: 1024
  disks:
    data1:
      hds: 4
    data2:
      hds: 4
    data3:
      hds: 4
    data4:
      hds: 4
